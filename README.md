# Docker and pyspark

_This is a simple project to setup pyspark with docker_


### Pre-requisites  ğŸ“‹

_For this project you need to have docker and docker compose installed on your computer._
_You can download and install docker in this link: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04-es_

### Setup ğŸ”§

_In this case our docker file  and docker-compose is ready, the only thing we have to do is to build the container_

_We build the container_

```
docker-compose up --build spark_dev_v
```

_If everything went well we should get a message in console_

![Spark_service](img/spark_service.png)



## Running the tests âš™ï¸

_in the project we have 3 notebooks where we do some basic operations in the dataframes and rdds_


![Example_notebbok](img/notebook.png)


## Built with ğŸ› ï¸


* [Docker](https://docs.docker.com/get-docker/) - ğŸ‹

* [Python](https://docs.python.org/3/) - ğŸ



## Expressions of Gratitude ğŸ

* Tell others about this project ğŸ“¢
* Invite a beer ğŸº or a coffee â˜• . 
* Say thank you publicly ğŸ¤“.
* etc.
